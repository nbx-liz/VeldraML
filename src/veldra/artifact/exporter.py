"""Artifact export helpers."""

from __future__ import annotations

import json
import subprocess
import sys
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

import numpy as np

from veldra.api.artifact import Artifact
from veldra.api.exceptions import VeldraValidationError
from veldra.config.io import save_run_config

_RUNTIME_PREDICT_PY = """\
from __future__ import annotations

from pathlib import Path
from typing import Any

import lightgbm as lgb
import pandas as pd


def load_booster(export_dir: str | Path) -> lgb.Booster:
    base = Path(export_dir)
    model_path = base / "model.lgb.txt"
    if not model_path.exists():
        raise FileNotFoundError(f"model file not found: {model_path}")
    return lgb.Booster(model_file=str(model_path))


def predict_frame(export_dir: str | Path, frame: pd.DataFrame) -> Any:
    booster = load_booster(export_dir)
    return booster.predict(frame)
"""

_PYTHON_EXPORT_README = """\
# Veldra Export Package (Python)

This export is generated by `veldra.api.runner.export(..., format="python")`.

## Contents
- `model.lgb.txt`
- `manifest.json`
- `run_config.yaml`
- `feature_schema.json`
- `metadata.json`
- `runtime_predict.py`

## Usage
```python
import pandas as pd
from runtime_predict import predict_frame

frame = pd.read_csv("input.csv")
pred = predict_frame(".", frame)
print(pred[:5])
```
"""


def _metadata_payload(artifact: Artifact, *, export_format: str) -> dict[str, Any]:
    payload: dict[str, Any] = {
        "run_id": artifact.manifest.run_id,
        "task_type": artifact.run_config.task.type,
        "export_format": export_format,
        "created_at_utc": datetime.now(timezone.utc).isoformat(),
        "project_version": artifact.manifest.project_version,
        "manifest_version": artifact.manifest.manifest_version,
        "config_version": artifact.run_config.config_version,
    }
    if artifact.run_config.task.type == "frontier":
        payload["frontier_alpha"] = float(
            artifact.feature_schema.get("frontier_alpha", artifact.run_config.frontier.alpha)
        )
    return payload


def export_python_package(artifact: Artifact, out_dir: str | Path) -> Path:
    """Export artifact into a lightweight Python inference package."""
    if artifact.model_text is None:
        raise VeldraValidationError("Artifact model is missing and cannot be exported.")
    if not artifact.feature_schema:
        raise VeldraValidationError("Artifact feature_schema is missing and cannot be exported.")

    target = Path(out_dir)
    target.mkdir(parents=True, exist_ok=True)
    (target / "manifest.json").write_text(
        artifact.manifest.model_dump_json(indent=2),
        encoding="utf-8",
    )
    save_run_config(artifact.run_config, target / "run_config.yaml")
    (target / "feature_schema.json").write_text(
        json.dumps(artifact.feature_schema, indent=2, sort_keys=True),
        encoding="utf-8",
    )
    (target / "model.lgb.txt").write_text(artifact.model_text, encoding="utf-8")
    (target / "metadata.json").write_text(
        json.dumps(_metadata_payload(artifact, export_format="python"), indent=2, sort_keys=True),
        encoding="utf-8",
    )
    (target / "runtime_predict.py").write_text(_RUNTIME_PREDICT_PY, encoding="utf-8")
    (target / "README.md").write_text(_PYTHON_EXPORT_README, encoding="utf-8")
    return target


def _load_onnx_toolchain() -> tuple[Any, Any]:
    try:
        import onnxmltools
        from onnxconverter_common.data_types import FloatTensorType
    except ModuleNotFoundError as exc:
        missing = getattr(exc, "name", "onnx toolchain package")
        raise VeldraValidationError(
            "ONNX export requires optional dependencies. "
            f"Missing package: '{missing}'. "
            "Install with: uv sync --extra export-onnx"
        ) from exc
    return onnxmltools, FloatTensorType


def export_onnx_model(artifact: Artifact, out_dir: str | Path) -> Path:
    """Export artifact model into ONNX format when optional deps are available."""
    if artifact.model_text is None:
        raise VeldraValidationError("Artifact model is missing and cannot be exported.")

    feature_names = artifact.feature_schema.get("feature_names")
    if not isinstance(feature_names, list) or not feature_names:
        raise VeldraValidationError(
            "feature_schema.feature_names is missing and cannot export ONNX."
        )

    onnxmltools, float_tensor_type = _load_onnx_toolchain()
    target = Path(out_dir)
    target.mkdir(parents=True, exist_ok=True)

    booster = artifact._get_booster()
    initial_types = [("input", float_tensor_type([None, len(feature_names)]))]
    try:
        onnx_model = onnxmltools.convert_lightgbm(booster, initial_types=initial_types)
    except Exception as exc:
        raise VeldraValidationError(
            "ONNX conversion failed"
            f" for task.type='{artifact.run_config.task.type}'. "
            "Check converter compatibility for the current LightGBM model and "
            "ensure optional dependencies are installed with: uv sync --extra export-onnx"
        ) from exc
    model_path = target / "model.onnx"
    try:
        with model_path.open("wb") as fp:
            fp.write(onnx_model.SerializeToString())
    except Exception as exc:
        raise VeldraValidationError(
            "Failed to serialize/write ONNX model artifact. "
            "Verify converter output compatibility and filesystem permissions."
        ) from exc

    optimization = {
        "onnx_optimized": False,
        "onnx_optimization_mode": None,
        "optimized_model_path": None,
        "size_before_bytes": int(model_path.stat().st_size),
        "size_after_bytes": None,
    }
    opt_cfg = artifact.run_config.export.onnx_optimization
    if opt_cfg.enabled:
        optimization = _optimize_onnx_model(
            model_path=model_path,
            mode=opt_cfg.mode or "dynamic_quant",
            out_dir=target,
        )

    (target / "metadata.json").write_text(
        json.dumps(
            {
                **_metadata_payload(artifact, export_format="onnx"),
                **optimization,
            },
            indent=2,
            sort_keys=True,
        ),
        encoding="utf-8",
    )
    return target


def _optimize_onnx_model(
    *,
    model_path: Path,
    mode: str,
    out_dir: Path,
) -> dict[str, Any]:
    if mode != "dynamic_quant":
        raise VeldraValidationError(
            f"Unsupported ONNX optimization mode '{mode}'. Supported: dynamic_quant"
        )

    try:
        from onnxruntime.quantization import QuantType, quantize_dynamic
    except ModuleNotFoundError as exc:
        missing = getattr(exc, "name", "onnxruntime")
        raise VeldraValidationError(
            "ONNX dynamic quantization requires optional dependencies. "
            f"Missing package: '{missing}'. "
            "Install with: uv sync --extra export-onnx"
        ) from exc

    optimized_path = out_dir / "model.optimized.onnx"
    try:
        quantize_dynamic(
            model_input=str(model_path),
            model_output=str(optimized_path),
            weight_type=QuantType.QInt8,
        )
    except Exception as exc:
        raise VeldraValidationError(
            "ONNX dynamic quantization failed. "
            "Verify converter/runtime compatibility for this model and ensure "
            "optional dependencies are installed with: uv sync --extra export-onnx"
        ) from exc

    if not optimized_path.exists():
        raise VeldraValidationError("ONNX optimization did not produce model.optimized.onnx.")

    size_before = int(model_path.stat().st_size)
    size_after = int(optimized_path.stat().st_size)
    return {
        "onnx_optimized": True,
        "onnx_optimization_mode": mode,
        "optimized_model_path": str(optimized_path),
        "size_before_bytes": size_before,
        "size_after_bytes": size_after,
    }


def _write_validation_report(
    out_dir: str | Path,
    *,
    mode: str,
    checks: list[dict[str, Any]],
    extra: dict[str, Any] | None = None,
) -> dict[str, Any]:
    target = Path(out_dir)
    passed = all(bool(item.get("ok")) for item in checks)
    payload: dict[str, Any] = {
        "validation_mode": mode,
        "validation_passed": passed,
        "created_at_utc": datetime.now(timezone.utc).isoformat(),
        "checks": checks,
    }
    if extra:
        payload.update(extra)
    report_path = target / "validation_report.json"
    report_path.write_text(
        json.dumps(payload, indent=2, sort_keys=True),
        encoding="utf-8",
    )
    summary = {
        "validation_mode": mode,
        "validation_passed": passed,
        "validation_report": str(report_path),
    }
    if extra:
        summary.update(extra)
    return summary


def _validate_python_export(export_dir: str | Path, artifact: Artifact) -> dict[str, Any]:
    target = Path(export_dir)
    checks: list[dict[str, Any]] = []
    required = [
        "manifest.json",
        "run_config.yaml",
        "feature_schema.json",
        "model.lgb.txt",
        "metadata.json",
        "runtime_predict.py",
        "README.md",
    ]
    for file_name in required:
        exists = (target / file_name).exists()
        checks.append({"name": f"required_file:{file_name}", "ok": exists})

    runtime_path = target / "runtime_predict.py"
    feature_names = artifact.feature_schema.get("feature_names")
    if runtime_path.exists():
        if not isinstance(feature_names, list) or not feature_names:
            checks.append(
                {
                    "name": "runtime_predict",
                    "ok": False,
                    "detail": "feature_schema.feature_names is missing or invalid",
                }
            )
        else:
            try:
                encoded_feature_names = json.dumps([str(name) for name in feature_names])
                probe_script = (
                    "import importlib.util, json, pandas as pd\n"
                    "spec = importlib.util.spec_from_file_location(\n"
                    "    'runtime_predict',\n"
                    "    'runtime_predict.py',\n"
                    ")\n"
                    "mod = importlib.util.module_from_spec(spec)\n"
                    "spec.loader.exec_module(mod)\n"
                    f"names = json.loads({json.dumps(encoded_feature_names)})\n"
                    "frame = pd.DataFrame({n: [0.0] for n in names}, columns=names)\n"
                    "pred = mod.predict_frame('.', frame)\n"
                    "print(len(pred) if hasattr(pred, '__len__') else -1)\n"
                )
                completed = subprocess.run(
                    [sys.executable, "-c", probe_script],
                    cwd=str(target),
                    capture_output=True,
                    text=True,
                    timeout=20,
                    check=False,
                )
                pred_len = (
                    int(completed.stdout.strip())
                    if completed.returncode == 0 and completed.stdout.strip().isdigit()
                    else None
                )
                detail = (
                    f"prediction_length={pred_len}"
                    if completed.returncode == 0
                    else (completed.stderr.strip() or completed.stdout.strip() or "probe failed")
                )
                checks.append(
                    {
                        "name": "runtime_predict",
                        "ok": completed.returncode == 0 and pred_len == 1,
                        "detail": detail,
                    }
                )
            except Exception as exc:
                checks.append(
                    {
                        "name": "runtime_predict",
                        "ok": False,
                        "detail": str(exc),
                    }
                )
    return _write_validation_report(target, mode="python", checks=checks)


def _validate_onnx_export(export_dir: str | Path, artifact: Artifact) -> dict[str, Any]:
    target = Path(export_dir)
    checks: list[dict[str, Any]] = []
    model_path = target / "model.onnx"
    metadata_path = target / "metadata.json"
    checks.append({"name": "required_file:model.onnx", "ok": model_path.exists()})
    checks.append({"name": "required_file:metadata.json", "ok": metadata_path.exists()})
    optimized_path = target / "model.optimized.onnx"
    checks.append(
        {
            "name": "optional_file:model.optimized.onnx",
            "ok": True,
            "detail": "present" if optimized_path.exists() else "not_present",
        }
    )

    optimization_extra: dict[str, Any] = {
        "onnx_optimized": False,
        "onnx_optimization_mode": None,
        "optimized_model_path": None,
        "size_before_bytes": int(model_path.stat().st_size) if model_path.exists() else None,
        "size_after_bytes": int(optimized_path.stat().st_size) if optimized_path.exists() else None,
    }
    if metadata_path.exists():
        try:
            meta = json.loads(metadata_path.read_text(encoding="utf-8"))
            optimization_extra["onnx_optimized"] = bool(meta.get("onnx_optimized", False))
            optimization_extra["onnx_optimization_mode"] = meta.get("onnx_optimization_mode")
            optimization_extra["optimized_model_path"] = meta.get("optimized_model_path")
            if meta.get("size_before_bytes") is not None:
                optimization_extra["size_before_bytes"] = int(meta["size_before_bytes"])
            if meta.get("size_after_bytes") is not None:
                optimization_extra["size_after_bytes"] = int(meta["size_after_bytes"])
        except Exception:
            checks.append(
                {
                    "name": "metadata_parse",
                    "ok": False,
                    "detail": "Failed to parse metadata.json",
                }
            )

    try:
        import onnx
        import onnxruntime as ort
    except ModuleNotFoundError as exc:
        checks.append(
            {
                "name": "onnx_runtime_import",
                "ok": False,
                "detail": (
                    f"Missing package '{getattr(exc, 'name', 'onnx runtime package')}'. "
                    "Install with: uv sync --extra export-onnx"
                ),
            }
        )
        return _write_validation_report(
            target, mode="onnx", checks=checks, extra=optimization_extra
        )

    if model_path.exists():
        try:
            model = onnx.load(str(model_path))
            onnx.checker.check_model(model)
            checks.append({"name": "onnx_model_check", "ok": True})
        except Exception as exc:
            checks.append({"name": "onnx_model_check", "ok": False, "detail": str(exc)})

        feature_names = artifact.feature_schema.get("feature_names")
        if not isinstance(feature_names, list) or not feature_names:
            checks.append(
                {
                    "name": "onnx_runtime_inference",
                    "ok": False,
                    "detail": "feature_schema.feature_names is missing or invalid",
                }
            )
        else:
            try:
                session = ort.InferenceSession(
                    str(model_path),
                    providers=["CPUExecutionProvider"],
                )
                input_name = session.get_inputs()[0].name
                sample = np.zeros((1, len(feature_names)), dtype=np.float32)
                outputs = session.run(None, {input_name: sample})
                checks.append(
                    {
                        "name": "onnx_runtime_inference",
                        "ok": len(outputs) > 0,
                        "detail": f"n_outputs={len(outputs)}",
                    }
                )
            except Exception as exc:
                checks.append(
                    {
                        "name": "onnx_runtime_inference",
                        "ok": False,
                        "detail": str(exc),
                    }
                )

    return _write_validation_report(target, mode="onnx", checks=checks, extra=optimization_extra)
