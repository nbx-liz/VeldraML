{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Frontier Analysis Workflow (Capacity Risk Focus)\n",
        "\n",
        "This notebook is tailored for frontier analysis: estimating the upper demand boundary for capacity planning.\n",
        "\n",
        "Analysis goals:\n",
        "- Interpret `frontier_pred` as required capacity guardrail\n",
        "- Validate whether `coverage` (actual <= frontier) is aligned with target alpha\n",
        "- Quantify shortfall risk with `u_hat` when actual exceeds the frontier\n",
        "- Compare operational what-if scenarios with `simulate`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Imports / Paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import asdict\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "from veldra.api import Artifact, evaluate, export, fit, predict, simulate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ROOT = Path.cwd()\n",
        "while not (ROOT / \"pyproject.toml\").exists() and ROOT.parent != ROOT:\n",
        "    ROOT = ROOT.parent\n",
        "\n",
        "OUT_DIR = ROOT / \"examples\" / \"out\" / \"notebook_frontier\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "TARGET_COL = \"peak_load\"\n",
        "ALPHA = 0.90\n",
        "\n",
        "print(f\"project_root={ROOT}\")\n",
        "print(f\"out_dir={OUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Synthetic Data Generation (Frontier-Oriented)\n",
        "\n",
        "- `peak_load` represents daily peak resource demand\n",
        "- Right-tail shock noise is introduced to emulate realistic overload risk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_frontier_base_data(n_samples: int = 2400, random_state: int = 42) -> pd.DataFrame:\n",
        "    rng = np.random.default_rng(random_state)\n",
        "\n",
        "    active_users = rng.integers(800, 8000, size=n_samples)\n",
        "    new_signups = rng.integers(10, 700, size=n_samples)\n",
        "    incidents_7d = rng.poisson(1.8, size=n_samples)\n",
        "    release_events_7d = rng.poisson(0.9, size=n_samples)\n",
        "    avg_latency_ms = rng.normal(180.0, 45.0, size=n_samples).clip(70, 450)\n",
        "    seasonality_idx = rng.uniform(0.85, 1.20, size=n_samples)\n",
        "\n",
        "    base = (\n",
        "        0.030 * active_users\n",
        "        + 0.12 * new_signups\n",
        "        + 6.0 * incidents_7d\n",
        "        + 3.0 * release_events_7d\n",
        "        + 0.08 * avg_latency_ms\n",
        "    ) * seasonality_idx\n",
        "\n",
        "    # Heteroscedastic upper-tail noise (frontier use case)\n",
        "    sigma = 8.0 + 0.004 * active_users + 2.2 * incidents_7d\n",
        "    shock = rng.lognormal(mean=2.0, sigma=0.45, size=n_samples)\n",
        "    noise = rng.normal(0.0, sigma, size=n_samples)\n",
        "    peak_load = np.clip(base + noise + 0.5 * shock, 1.0, None)\n",
        "\n",
        "    return pd.DataFrame(\n",
        "        {\n",
        "            \"active_users\": active_users,\n",
        "            \"new_signups\": new_signups,\n",
        "            \"incidents_7d\": incidents_7d,\n",
        "            \"release_events_7d\": release_events_7d,\n",
        "            \"avg_latency_ms\": avg_latency_ms,\n",
        "            \"seasonality_idx\": seasonality_idx,\n",
        "            TARGET_COL: peak_load,\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "def generate_frontier_drifted_data(n_samples: int = 1200, random_state: int = 99) -> pd.DataFrame:\n",
        "    df = generate_frontier_base_data(n_samples=n_samples, random_state=random_state)\n",
        "    rng = np.random.default_rng(random_state + 31)\n",
        "\n",
        "    # Drift: latency and incidents worsen.\n",
        "    df[\"incidents_7d\"] = df[\"incidents_7d\"] + rng.poisson(1.1, size=n_samples)\n",
        "    df[\"avg_latency_ms\"] = (df[\"avg_latency_ms\"] + rng.normal(22, 12, size=n_samples)).clip(70, 600)\n",
        "\n",
        "    base = (\n",
        "        0.031 * df[\"active_users\"].to_numpy(dtype=float)\n",
        "        + 0.12 * df[\"new_signups\"].to_numpy(dtype=float)\n",
        "        + 6.8 * df[\"incidents_7d\"].to_numpy(dtype=float)\n",
        "        + 3.2 * df[\"release_events_7d\"].to_numpy(dtype=float)\n",
        "        + 0.09 * df[\"avg_latency_ms\"].to_numpy(dtype=float)\n",
        "    ) * df[\"seasonality_idx\"].to_numpy(dtype=float)\n",
        "\n",
        "    sigma = 10.0 + 0.0045 * df[\"active_users\"].to_numpy(dtype=float) + 2.5 * df[\"incidents_7d\"].to_numpy(dtype=float)\n",
        "    shock = rng.lognormal(mean=2.1, sigma=0.50, size=n_samples)\n",
        "    noise = rng.normal(0.0, sigma, size=n_samples)\n",
        "    df[TARGET_COL] = np.clip(base + noise + 0.65 * shock, 1.0, None)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = generate_frontier_base_data(n_samples=2400, random_state=42)\n",
        "test_df = generate_frontier_drifted_data(n_samples=1200, random_state=77)\n",
        "\n",
        "TRAIN_PATH = OUT_DIR / \"frontier_train.parquet\"\n",
        "TEST_PATH = OUT_DIR / \"frontier_test.parquet\"\n",
        "train_df.to_parquet(TRAIN_PATH, index=False)\n",
        "test_df.to_parquet(TEST_PATH, index=False)\n",
        "\n",
        "display(train_df.head())\n",
        "display(test_df.head())\n",
        "print(f\"train_path={TRAIN_PATH}\")\n",
        "print(f\"test_path={TEST_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Train Frontier Model (`alpha=0.90`) and Evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"config_version\": 1,\n",
        "    \"task\": {\"type\": \"frontier\"},\n",
        "    \"frontier\": {\"alpha\": ALPHA},\n",
        "    \"data\": {\"path\": str(TRAIN_PATH), \"target\": TARGET_COL},\n",
        "    \"split\": {\"type\": \"kfold\", \"n_splits\": 5, \"seed\": 42},\n",
        "    \"train\": {\"seed\": 42},\n",
        "    \"export\": {\"artifact_dir\": str(OUT_DIR / \"artifacts\")},\n",
        "}\n",
        "\n",
        "run = fit(config)\n",
        "artifact = Artifact.load(run.artifact_path)\n",
        "\n",
        "ev_train = evaluate(artifact, train_df)\n",
        "ev_test = evaluate(artifact, test_df)\n",
        "\n",
        "display(pd.DataFrame([ev_train.metrics, ev_test.metrics], index=[\"train\", \"test\"]))\n",
        "print(f\"artifact_path={run.artifact_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_frontier_pred_df(df: pd.DataFrame, pred_df: pd.DataFrame, split_name: str, alpha: float) -> pd.DataFrame:\n",
        "    out = pd.DataFrame({\n",
        "        \"split\": split_name,\n",
        "        \"actual\": df[TARGET_COL].to_numpy(dtype=float),\n",
        "        \"frontier_pred\": pred_df[\"frontier_pred\"].to_numpy(dtype=float),\n",
        "    })\n",
        "    out[\"margin\"] = out[\"frontier_pred\"] - out[\"actual\"]\n",
        "    out[\"u_hat\"] = np.maximum(0.0, -out[\"margin\"])  # capacity shortfall when actual exceeds frontier\n",
        "    out[\"covered\"] = (out[\"actual\"] <= out[\"frontier_pred\"]).astype(int)\n",
        "    out[\"alpha\"] = alpha\n",
        "    return out\n",
        "\n",
        "pred_train = predict(artifact, train_df.drop(columns=[TARGET_COL])).data\n",
        "pred_test = predict(artifact, test_df.drop(columns=[TARGET_COL])).data\n",
        "\n",
        "pred_train_df = build_frontier_pred_df(train_df, pred_train, \"train\", ALPHA)\n",
        "pred_test_df = build_frontier_pred_df(test_df, pred_test, \"test\", ALPHA)\n",
        "pred_all = pd.concat([pred_train_df, pred_test_df], ignore_index=True)\n",
        "\n",
        "display(pred_all.head())\n",
        "display(pred_all.groupby(\"split\")[[\"covered\", \"u_hat\", \"margin\"]].mean())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Frontier-Specific Diagnostics\n",
        "\n",
        "Focus metrics:\n",
        "- `coverage`: ratio of points under the frontier line\n",
        "- `margin`: `frontier_pred - actual`\n",
        "- `u_hat`: shortfall amount when demand exceeds frontier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_frontier_actual_vs_pred(pred_df: pd.DataFrame) -> None:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=False, sharey=False)\n",
        "    for ax, (split_name, frame) in zip(axes, pred_df.groupby(\"split\")):\n",
        "        ax.scatter(frame[\"actual\"], frame[\"frontier_pred\"], s=10, alpha=0.35)\n",
        "        lo = min(frame[\"actual\"].min(), frame[\"frontier_pred\"].min())\n",
        "        hi = max(frame[\"actual\"].max(), frame[\"frontier_pred\"].max())\n",
        "        ax.plot([lo, hi], [lo, hi], \"k--\", linewidth=1)\n",
        "        ax.set_title(f\"{split_name}: actual vs frontier_pred\")\n",
        "        ax.set_xlabel(\"actual\")\n",
        "        ax.set_ylabel(\"frontier_pred\")\n",
        "        ax.grid(alpha=0.25)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_frontier_residuals(pred_df: pd.DataFrame) -> None:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    for split_name, frame in pred_df.groupby(\"split\"):\n",
        "        axes[0].hist(frame[\"margin\"], bins=35, alpha=0.5, label=split_name)\n",
        "    axes[0].axvline(0.0, color=\"k\", linestyle=\"--\", linewidth=1)\n",
        "    axes[0].set_title(\"Margin distribution (frontier_pred - actual)\")\n",
        "    axes[0].set_xlabel(\"margin\")\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(alpha=0.25)\n",
        "\n",
        "    for split_name, frame in pred_df.groupby(\"split\"):\n",
        "        axes[1].hist(frame[\"u_hat\"], bins=35, alpha=0.5, label=split_name)\n",
        "    axes[1].set_title(\"Shortfall distribution (u_hat)\")\n",
        "    axes[1].set_xlabel(\"u_hat\")\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(alpha=0.25)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_frontier_actual_vs_pred(pred_all)\n",
        "plot_frontier_residuals(pred_all)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coverage and shortfall by risk segment (operations-focused)\n",
        "segment_df = test_df[[\"incidents_7d\", \"avg_latency_ms\"]].copy()\n",
        "segment_df[\"incidents_band\"] = pd.cut(segment_df[\"incidents_7d\"], bins=[-1,1,3,6,100], labels=[\"low\",\"mid\",\"high\",\"extreme\"])\n",
        "segment_df[\"latency_band\"] = pd.qcut(segment_df[\"avg_latency_ms\"], q=4, labels=[\"Q1\",\"Q2\",\"Q3\",\"Q4\"])\n",
        "\n",
        "frontier_ops = pred_test_df.join(segment_df[[\"incidents_band\", \"latency_band\"]])\n",
        "coverage_by_incident = frontier_ops.groupby(\"incidents_band\", observed=False)[[\"covered\", \"u_hat\"]].mean().sort_index()\n",
        "coverage_by_latency = frontier_ops.groupby(\"latency_band\", observed=False)[[\"covered\", \"u_hat\"]].mean().sort_index()\n",
        "\n",
        "display(coverage_by_incident)\n",
        "display(coverage_by_latency)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_lgb_importance(artifact_obj: Artifact) -> pd.DataFrame:\n",
        "    if not artifact_obj.model_text:\n",
        "        raise ValueError(\"Artifact model_text is missing.\")\n",
        "    booster = lgb.Booster(model_str=artifact_obj.model_text)\n",
        "    gain = booster.feature_importance(importance_type=\"gain\")\n",
        "    split = booster.feature_importance(importance_type=\"split\")\n",
        "    names = booster.feature_name()\n",
        "    out = pd.DataFrame({\"feature\": names, \"gain\": gain, \"split\": split})\n",
        "    out = out.sort_values(\"gain\", ascending=False).reset_index(drop=True)\n",
        "    return out\n",
        "\n",
        "importance_df = compute_lgb_importance(artifact)\n",
        "display(importance_df.head(15))\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.barh(importance_df.head(12)[\"feature\"][::-1], importance_df.head(12)[\"gain\"][::-1])\n",
        "plt.title(\"Feature Importance (gain)\")\n",
        "plt.xlabel(\"gain\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LightGBM pred_contrib provides SHAP-like additive contributions.\n",
        "def compute_shap_summary(artifact_obj: Artifact, x_sample: pd.DataFrame) -> pd.DataFrame:\n",
        "    if not artifact_obj.model_text:\n",
        "        raise ValueError(\"Artifact model_text is missing.\")\n",
        "    booster = lgb.Booster(model_str=artifact_obj.model_text)\n",
        "    contrib = booster.predict(x_sample, pred_contrib=True)\n",
        "    columns = booster.feature_name() + [\"bias\"]\n",
        "    contrib_df = pd.DataFrame(contrib, columns=columns)\n",
        "    mean_abs = contrib_df.drop(columns=[\"bias\"]).abs().mean().sort_values(ascending=False)\n",
        "    out = mean_abs.rename(\"mean_abs_shap_like\").reset_index()\n",
        "    out.columns = [\"feature\", \"mean_abs_shap_like\"]\n",
        "    return out\n",
        "\n",
        "x_test = test_df.drop(columns=[TARGET_COL])\n",
        "shap_summary_df = compute_shap_summary(artifact, x_test)\n",
        "display(shap_summary_df.head(15))\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.barh(shap_summary_df.head(12)[\"feature\"][::-1], shap_summary_df.head(12)[\"mean_abs_shap_like\"][::-1])\n",
        "plt.title(\"SHAP-like summary (LightGBM pred_contrib)\")\n",
        "plt.xlabel(\"mean(|contribution|)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) What-If Simulation for Capacity Planning\n",
        "\n",
        "Scenario examples:\n",
        "- `incident_spike`: more incidents and latency degradation\n",
        "- `mitigation_playbook`: operational improvements to reduce risk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scenarios = [\n",
        "    {\n",
        "        \"name\": \"incident_spike\",\n",
        "        \"actions\": [\n",
        "            {\"op\": \"add\", \"column\": \"incidents_7d\", \"value\": 2},\n",
        "            {\"op\": \"add\", \"column\": \"avg_latency_ms\", \"value\": 25},\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"mitigation_playbook\",\n",
        "        \"actions\": [\n",
        "            {\"op\": \"add\", \"column\": \"incidents_7d\", \"value\": -1},\n",
        "            {\"op\": \"mul\", \"column\": \"avg_latency_ms\", \"value\": 0.92},\n",
        "            {\"op\": \"mul\", \"column\": \"release_events_7d\", \"value\": 0.90},\n",
        "        ],\n",
        "    },\n",
        "]\n",
        "\n",
        "sim = simulate(artifact, test_df.copy(), scenarios)\n",
        "sim_df = sim.data\n",
        "\n",
        "display(sim_df.head())\n",
        "summary = (\n",
        "    sim_df.groupby(\"scenario\")[[\"delta_pred\"]]\n",
        "    .agg([\"mean\", \"median\", \"min\", \"max\"])\n",
        ")\n",
        "display(summary)\n",
        "\n",
        "if \"delta_u_hat\" in sim_df.columns:\n",
        "    risk_summary = sim_df.groupby(\"scenario\")[[\"delta_u_hat\"]].mean().rename(columns={\"delta_u_hat\": \"mean_delta_u_hat\"})\n",
        "    display(risk_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export portable package (python format)\n",
        "exp = export(artifact, format=\"python\")\n",
        "print(asdict(exp))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Frontier-Specific Conclusion Template\n",
        "\n",
        "- If coverage is materially below alpha, frontier may be too optimistic.\n",
        "- Use `u_hat` mean and high quantiles to estimate required buffer capacity.\n",
        "- Rank intervention scenarios by `delta_pred` and `delta_u_hat` to prioritize actions.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}