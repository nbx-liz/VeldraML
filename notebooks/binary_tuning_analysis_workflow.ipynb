{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Binary + Tune Analysis Workflow\n",
        "\n",
        "This notebook demonstrates a practical binary-analysis scenario using Veldra:\n",
        "- Generate synthetic churn-risk data in the notebook\n",
        "- Train a calibrated binary model\n",
        "- Tune hyperparameters and compare baseline vs tuned quality\n",
        "- Visualize ROC, calibration, confusion matrix, and error table\n",
        "\n",
        "Use case:\n",
        "Prioritize customer retention outreach under limited budget.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Setup\n",
        "- No external dataset path is required.\n",
        "- The notebook writes temporary train/test parquet files under `examples/out/notebook_binary_tune/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import asdict\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from sklearn.metrics import (\n",
        "    ConfusionMatrixDisplay,\n",
        "    RocCurveDisplay,\n",
        "    brier_score_loss,\n",
        "    log_loss,\n",
        "    roc_auc_score,\n",
        ")\n",
        "\n",
        "from veldra.api import Artifact, evaluate, fit, predict, tune\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ROOT = Path.cwd()\n",
        "while not (ROOT / \"pyproject.toml\").exists() and ROOT.parent != ROOT:\n",
        "    ROOT = ROOT.parent\n",
        "\n",
        "OUT_DIR = ROOT / \"examples\" / \"out\" / \"notebook_binary_tune\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TARGET_COL = \"churned\"\n",
        "RNG_SEED = 42\n",
        "\n",
        "print(f\"project_root={ROOT}\")\n",
        "print(f\"out_dir={OUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Generate synthetic binary data (base + drift)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _sigmoid(x: np.ndarray) -> np.ndarray:\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "\n",
        "def generate_binary_base_data(n_samples: int = 4000, random_state: int = 42) -> pd.DataFrame:\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    tenure_months = rng.integers(1, 72, size=n_samples)\n",
        "    monthly_spend = rng.normal(120.0, 35.0, size=n_samples).clip(20, 350)\n",
        "    support_tickets_30d = rng.poisson(1.6, size=n_samples)\n",
        "    feature_usage_ratio = rng.beta(3.0, 2.5, size=n_samples)\n",
        "    discount_rate = rng.choice([0.0, 0.05, 0.1, 0.15], size=n_samples, p=[0.25, 0.35, 0.25, 0.15])\n",
        "    nps = rng.normal(25, 22, size=n_samples).clip(-100, 100)\n",
        "\n",
        "    logit = (\n",
        "        -1.8\n",
        "        - 0.025 * tenure_months\n",
        "        + 0.012 * monthly_spend\n",
        "        + 0.38 * support_tickets_30d\n",
        "        - 2.1 * feature_usage_ratio\n",
        "        - 2.5 * discount_rate\n",
        "        - 0.01 * nps\n",
        "    )\n",
        "    p = _sigmoid(logit)\n",
        "    churned = rng.binomial(1, p)\n",
        "\n",
        "    return pd.DataFrame(\n",
        "        {\n",
        "            \"tenure_months\": tenure_months,\n",
        "            \"monthly_spend\": monthly_spend,\n",
        "            \"support_tickets_30d\": support_tickets_30d,\n",
        "            \"feature_usage_ratio\": feature_usage_ratio,\n",
        "            \"discount_rate\": discount_rate,\n",
        "            \"nps\": nps,\n",
        "            TARGET_COL: churned,\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "def generate_binary_drifted_data(n_samples: int = 1500, random_state: int = 99) -> pd.DataFrame:\n",
        "    df = generate_binary_base_data(n_samples=n_samples, random_state=random_state)\n",
        "    rng = np.random.default_rng(random_state + 7)\n",
        "\n",
        "    # Drift: support load up, usage slightly down, and spend up.\n",
        "    df[\"support_tickets_30d\"] = df[\"support_tickets_30d\"] + rng.poisson(0.9, size=n_samples)\n",
        "    df[\"feature_usage_ratio\"] = (df[\"feature_usage_ratio\"] - rng.normal(0.05, 0.03, size=n_samples)).clip(0, 1)\n",
        "    df[\"monthly_spend\"] = (df[\"monthly_spend\"] + rng.normal(12, 7, size=n_samples)).clip(20, 400)\n",
        "\n",
        "    # Recompute target with harder churn environment.\n",
        "    logit = (\n",
        "        -1.4\n",
        "        - 0.02 * df[\"tenure_months\"].to_numpy(dtype=float)\n",
        "        + 0.013 * df[\"monthly_spend\"].to_numpy(dtype=float)\n",
        "        + 0.43 * df[\"support_tickets_30d\"].to_numpy(dtype=float)\n",
        "        - 1.9 * df[\"feature_usage_ratio\"].to_numpy(dtype=float)\n",
        "        - 2.2 * df[\"discount_rate\"].to_numpy(dtype=float)\n",
        "        - 0.009 * df[\"nps\"].to_numpy(dtype=float)\n",
        "    )\n",
        "    p = _sigmoid(logit)\n",
        "    df[TARGET_COL] = rng.binomial(1, p)\n",
        "    return df\n",
        "\n",
        "\n",
        "train_df = generate_binary_base_data(n_samples=4000, random_state=RNG_SEED)\n",
        "test_df = generate_binary_drifted_data(n_samples=1500, random_state=RNG_SEED + 57)\n",
        "\n",
        "display(train_df.head())\n",
        "display(test_df.head())\n",
        "print(\"train_positive_rate=\", round(float(train_df[TARGET_COL].mean()), 4))\n",
        "print(\"test_positive_rate=\", round(float(test_df[TARGET_COL].mean()), 4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_PATH = OUT_DIR / \"binary_train.parquet\"\n",
        "TEST_PATH = OUT_DIR / \"binary_test.parquet\"\n",
        "train_df.to_parquet(TRAIN_PATH, index=False)\n",
        "test_df.to_parquet(TEST_PATH, index=False)\n",
        "\n",
        "print(f\"train_parquet={TRAIN_PATH}\")\n",
        "print(f\"test_parquet={TEST_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Baseline fit/evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline_config = {\n",
        "    \"config_version\": 1,\n",
        "    \"task\": {\"type\": \"binary\"},\n",
        "    \"data\": {\"path\": str(TRAIN_PATH), \"target\": TARGET_COL},\n",
        "    \"split\": {\"type\": \"stratified\", \"n_splits\": 5, \"seed\": 42},\n",
        "    \"train\": {\"seed\": 42, \"lgb_params\": {\"num_leaves\": 31, \"learning_rate\": 0.05}},\n",
        "    \"postprocess\": {\"calibration\": \"platt\"},\n",
        "    \"export\": {\"artifact_dir\": str(OUT_DIR / \"artifacts\")},\n",
        "}\n",
        "\n",
        "baseline_run = fit(baseline_config)\n",
        "baseline_artifact = Artifact.load(baseline_run.artifact_path)\n",
        "\n",
        "baseline_eval_train = evaluate(baseline_artifact, train_df)\n",
        "baseline_eval_test = evaluate(baseline_artifact, test_df)\n",
        "\n",
        "print(\"baseline_run_id=\", baseline_run.run_id)\n",
        "print(\"baseline_artifact=\", baseline_run.artifact_path)\n",
        "display(pd.DataFrame([baseline_eval_train.metrics, baseline_eval_test.metrics], index=[\"train\", \"test\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Hyperparameter tuning (binary)\n",
        "- Objective defaults to `auc`\n",
        "- This example uses a small trial count for notebook speed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tune_config = {\n",
        "    \"config_version\": 1,\n",
        "    \"task\": {\"type\": \"binary\"},\n",
        "    \"data\": {\"path\": str(TRAIN_PATH), \"target\": TARGET_COL},\n",
        "    \"split\": {\"type\": \"stratified\", \"n_splits\": 5, \"seed\": 42},\n",
        "    \"train\": {\"seed\": 42},\n",
        "    \"tuning\": {\n",
        "        \"enabled\": True,\n",
        "        \"n_trials\": 12,\n",
        "        \"preset\": \"fast\",\n",
        "        \"objective\": \"auc\",\n",
        "        \"study_name\": \"notebook_binary_tune_demo\",\n",
        "        # resume=True keeps the notebook rerunnable without duplicate-study errors.\n",
        "        \"resume\": True,\n",
        "        \"log_level\": \"INFO\",\n",
        "    },\n",
        "    \"postprocess\": {\"calibration\": \"platt\"},\n",
        "    \"export\": {\"artifact_dir\": str(OUT_DIR / \"artifacts\")},\n",
        "}\n",
        "\n",
        "tune_result = tune(tune_config)\n",
        "print(\"best_score=\", tune_result.best_score)\n",
        "print(\"best_params=\", tune_result.best_params)\n",
        "print(\"summary_path=\", tune_result.metadata.get(\"summary_path\"))\n",
        "print(\"trials_path=\", tune_result.metadata.get(\"trials_path\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trials_df = pd.read_parquet(tune_result.metadata[\"trials_path\"])\n",
        "display(trials_df.tail())\n",
        "\n",
        "x_col = \"trial_number\" if \"trial_number\" in trials_df.columns else \"number\"\n",
        "y_col = \"value\" if \"value\" in trials_df.columns else \"objective_value\"\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7, 4))\n",
        "ax.plot(trials_df[x_col], trials_df[y_col], marker=\"o\", alpha=0.8)\n",
        "ax.set_title(\"Tuning Trial Objective History\")\n",
        "ax.set_xlabel(x_col)\n",
        "ax.set_ylabel(y_col)\n",
        "ax.grid(alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Train tuned model and compare baseline vs tuned\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tuned_config = {\n",
        "    **baseline_config,\n",
        "    \"train\": {\n",
        "        \"seed\": 42,\n",
        "        \"lgb_params\": {**tune_result.best_params},\n",
        "    },\n",
        "}\n",
        "\n",
        "tuned_run = fit(tuned_config)\n",
        "tuned_artifact = Artifact.load(tuned_run.artifact_path)\n",
        "\n",
        "tuned_eval_train = evaluate(tuned_artifact, train_df)\n",
        "tuned_eval_test = evaluate(tuned_artifact, test_df)\n",
        "\n",
        "comparison_df = pd.DataFrame(\n",
        "    [\n",
        "        {\"model\": \"baseline\", \"split\": \"train\", **baseline_eval_train.metrics},\n",
        "        {\"model\": \"baseline\", \"split\": \"test\", **baseline_eval_test.metrics},\n",
        "        {\"model\": \"tuned\", \"split\": \"train\", **tuned_eval_train.metrics},\n",
        "        {\"model\": \"tuned\", \"split\": \"test\", **tuned_eval_test.metrics},\n",
        "    ]\n",
        ")\n",
        "display(comparison_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Prediction diagnostics (table + charts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_pred_table(artifact: Artifact, df: pd.DataFrame, split_name: str) -> pd.DataFrame:\n",
        "    pred = predict(artifact, df.drop(columns=[TARGET_COL])).data\n",
        "    out = pd.DataFrame(\n",
        "        {\n",
        "            \"split\": split_name,\n",
        "            \"actual\": df[TARGET_COL].to_numpy(dtype=int),\n",
        "            \"p_cal\": pred[\"p_cal\"].to_numpy(dtype=float),\n",
        "            \"p_raw\": pred[\"p_raw\"].to_numpy(dtype=float),\n",
        "            \"label_pred\": pred[\"label_pred\"].to_numpy(dtype=int),\n",
        "        }\n",
        "    )\n",
        "    out[\"error\"] = out[\"actual\"] - out[\"p_cal\"]\n",
        "    out[\"abs_error\"] = np.abs(out[\"error\"])\n",
        "    return out\n",
        "\n",
        "baseline_train_pred = build_pred_table(baseline_artifact, train_df, \"train\")\n",
        "baseline_test_pred = build_pred_table(baseline_artifact, test_df, \"test\")\n",
        "baseline_pred_df = pd.concat([baseline_train_pred, baseline_test_pred], ignore_index=True)\n",
        "\n",
        "display(baseline_pred_df.head())\n",
        "display(\n",
        "    baseline_pred_df.groupby(\"split\")[[\"p_cal\", \"actual\", \"abs_error\"]]\n",
        "    .mean()\n",
        "    .rename(columns={\"p_cal\": \"mean_p_cal\", \"actual\": \"positive_rate\"})\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "for split_name, frame in baseline_pred_df.groupby(\"split\"):\n",
        "    RocCurveDisplay.from_predictions(frame[\"actual\"], frame[\"p_cal\"], ax=axes[0], name=split_name)\n",
        "axes[0].set_title(\"ROC (Baseline)\")\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "for split_name, frame in baseline_pred_df.groupby(\"split\"):\n",
        "    axes[1].hist(frame[\"error\"], bins=30, alpha=0.5, label=split_name)\n",
        "axes[1].set_title(\"Prediction Error Distribution (actual - p_cal)\")\n",
        "axes[1].set_xlabel(\"error\")\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix on test split using thresholded label_pred.\n",
        "ConfusionMatrixDisplay.from_predictions(\n",
        "    baseline_test_pred[\"actual\"],\n",
        "    baseline_test_pred[\"label_pred\"],\n",
        ")\n",
        "plt.title(\"Confusion Matrix (Baseline / Test)\")\n",
        "plt.show()\n",
        "\n",
        "print(\"test_auc=\", round(roc_auc_score(baseline_test_pred[\"actual\"], baseline_test_pred[\"p_cal\"]), 4))\n",
        "print(\"test_logloss=\", round(log_loss(baseline_test_pred[\"actual\"], baseline_test_pred[\"p_cal\"]), 4))\n",
        "print(\"test_brier=\", round(brier_score_loss(baseline_test_pred[\"actual\"], baseline_test_pred[\"p_cal\"]), 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Operational interpretation\n",
        "\n",
        "Suggested interpretation flow:\n",
        "1. Use tuned model if test AUC/logloss/Brier are consistently better.\n",
        "2. Segment by `p_cal` deciles and prioritize outreach in highest-risk segments.\n",
        "3. Compare train/test error distributions to monitor drift and recalibration needs.\n",
        "4. Re-run tune periodically when drift increases (e.g., monthly).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Persist notebook-friendly outputs.\n",
        "summary_path = OUT_DIR / \"binary_tune_notebook_summary.json\"\n",
        "summary_payload = {\n",
        "    \"baseline_run\": asdict(baseline_run),\n",
        "    \"tuned_run\": asdict(tuned_run),\n",
        "    \"tune_result\": asdict(tune_result),\n",
        "    \"baseline_eval_test\": asdict(baseline_eval_test),\n",
        "    \"tuned_eval_test\": asdict(tuned_eval_test),\n",
        "}\n",
        "pd.Series(summary_payload).to_json(summary_path, force_ascii=False, indent=2)\n",
        "print(f\"summary_json={summary_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}