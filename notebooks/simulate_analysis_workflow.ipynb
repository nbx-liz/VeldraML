{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# Simulation Analysis Workflow with Veldra\n",
    "\n",
    "This notebook shows a practical what-if analysis workflow using `simulate()` on top of a trained regression artifact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## Business Case\n",
    "\n",
    "Assume a SaaS team wants to estimate the impact of product and support interventions before rollout.\n",
    "We compare scenario-level uplift on predicted LTV and identify which customer segments gain most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from veldra.api import Artifact, evaluate, fit, simulate\n",
    "\n",
    "\n",
    "def _resolve_repo_root(start: Path) -> Path:\n",
    "    current = start.resolve()\n",
    "    candidates = [current, *current.parents]\n",
    "    for base in candidates:\n",
    "        if (base / \"pyproject.toml\").exists() and (base / \"examples\").exists():\n",
    "            return base\n",
    "    return start.resolve()\n",
    "\n",
    "ROOT = _resolve_repo_root(Path.cwd())\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "OUT_DIR = ROOT / \"examples\" / \"out\" / \"notebook_simulate\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TARGET_COL = \"target_ltv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-notebook synthetic data generator (self-contained).\n",
    "def generate_saas_ltv_data(n_samples: int = 3000, random_state: int = 42) -> pd.DataFrame:\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    company_size = rng.choice([\"Small\", \"Medium\", \"Enterprise\"], size=n_samples, p=[0.6, 0.3, 0.1])\n",
    "    login_days = np.clip(rng.normal(10, 8, n_samples), 0, 30).astype(int)\n",
    "    feature_usage = rng.exponential(scale=55, size=n_samples).astype(int)\n",
    "    support_tickets = np.clip(\n",
    "        rng.poisson(lam=np.maximum(login_days / 10, 0.1)),\n",
    "        0,\n",
    "        12,\n",
    "    ).astype(float)\n",
    "    nps = rng.integers(0, 11, size=n_samples).astype(float)\n",
    "    nps[rng.random(n_samples) < 0.25] = np.nan\n",
    "\n",
    "    base_ltv_map = {\"Small\": 1000, \"Medium\": 5200, \"Enterprise\": 19000}\n",
    "    base_val = np.array([base_ltv_map[s] for s in company_size], dtype=float)\n",
    "    effect_login = 460 * np.log1p(login_days)\n",
    "    effect_usage = 11 * feature_usage\n",
    "    effect_support = -420 * support_tickets\n",
    "    nps_filled = np.nan_to_num(nps, nan=7.0)\n",
    "    effect_nps = 120 * (nps_filled - 6.0)\n",
    "    noise = rng.normal(0, 0.22, n_samples)\n",
    "    log_ltv = (\n",
    "        np.log(base_val)\n",
    "        + (effect_login + effect_usage + effect_support + effect_nps) / 5200\n",
    "        + noise\n",
    "    )\n",
    "    target_ltv = np.round(np.exp(log_ltv), -2)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"company_size\": company_size,\n",
    "        \"login_days\": login_days,\n",
    "        \"feature_usage_count\": feature_usage,\n",
    "        \"support_tickets\": support_tickets,\n",
    "        \"nps_score\": nps,\n",
    "        TARGET_COL: target_ltv,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train/test datasets and keep context columns for segment drill-down.\n",
    "raw_df = generate_saas_ltv_data(n_samples=3000, random_state=42)\n",
    "train_df, test_df = train_test_split(raw_df, test_size=0.25, random_state=42)\n",
    "\n",
    "def _prepare_model_frame(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for col in out.drop(columns=[target_col]).select_dtypes(include=[\"object\"]).columns:\n",
    "        out[col] = out[col].astype(\"category\")\n",
    "    out[target_col] = out[target_col].astype(float)\n",
    "    return out\n",
    "\n",
    "train_model_df = _prepare_model_frame(train_df, TARGET_COL)\n",
    "test_model_df = _prepare_model_frame(test_df, TARGET_COL)\n",
    "for col in train_model_df.drop(columns=[TARGET_COL]).select_dtypes(include=[\"category\"]).columns:\n",
    "    if col in test_model_df.columns:\n",
    "        test_model_df[col] = test_model_df[col].astype(\"category\")\n",
    "        test_model_df[col] = test_model_df[col].cat.set_categories(\n",
    "            train_model_df[col].cat.categories\n",
    "        )\n",
    "\n",
    "TRAIN_PATH = OUT_DIR / \"simulate_train.parquet\"\n",
    "train_model_df.to_parquet(TRAIN_PATH, index=False)\n",
    "\n",
    "test_context = test_model_df[\n",
    "    [\"company_size\", \"login_days\", \"feature_usage_count\", \"support_tickets\"]\n",
    "].copy()\n",
    "display(train_model_df.head())\n",
    "display(test_model_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline model and evaluate baseline performance.\n",
    "config = {\n",
    "    \"config_version\": 1,\n",
    "    \"task\": {\"type\": \"regression\"},\n",
    "    \"data\": {\"path\": str(TRAIN_PATH), \"target\": TARGET_COL},\n",
    "    \"split\": {\"type\": \"kfold\", \"n_splits\": 5, \"seed\": 42},\n",
    "    \"train\": {\"seed\": 42},\n",
    "    \"export\": {\"artifact_dir\": str(OUT_DIR / \"artifacts\")},\n",
    "}\n",
    "\n",
    "run_result = fit(config)\n",
    "artifact = Artifact.load(run_result.artifact_path)\n",
    "baseline_eval = evaluate(artifact, test_model_df)\n",
    "baseline_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define intervention scenarios and run simulation.\n",
    "test_x = test_model_df.drop(columns=[TARGET_COL])\n",
    "\n",
    "scenarios = [\n",
    "    {\n",
    "        \"name\": \"onboarding_boost\",\n",
    "        \"actions\": [\n",
    "            {\"op\": \"add\", \"column\": \"login_days\", \"value\": 4},\n",
    "            {\"op\": \"add\", \"column\": \"feature_usage_count\", \"value\": 20},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"support_quality_improvement\",\n",
    "        \"actions\": [\n",
    "            {\"op\": \"add\", \"column\": \"support_tickets\", \"value\": -1.5},\n",
    "            {\"op\": \"clip\", \"column\": \"support_tickets\", \"min\": 0.0, \"max\": 12.0},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"combined_program\",\n",
    "        \"actions\": [\n",
    "            {\"op\": \"add\", \"column\": \"login_days\", \"value\": 3},\n",
    "            {\"op\": \"add\", \"column\": \"feature_usage_count\", \"value\": 15},\n",
    "            {\"op\": \"add\", \"column\": \"support_tickets\", \"value\": -1.0},\n",
    "            {\"op\": \"clip\", \"column\": \"support_tickets\", \"min\": 0.0, \"max\": 12.0},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "sim_result = simulate(artifact, test_x, scenarios)\n",
    "sim_df = sim_result.data.copy()\n",
    "sim_df = sim_df.merge(test_context, left_on=\"row_id\", right_index=True, how=\"left\")\n",
    "display(sim_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario KPI table: uplift, win-rate, and risk summary.\n",
    "scenario_kpi = (\n",
    "    sim_df.groupby(\"scenario\")\n",
    "    .agg(\n",
    "        mean_base=(\"base_pred\", \"mean\"),\n",
    "        mean_scenario=(\"scenario_pred\", \"mean\"),\n",
    "        mean_uplift=(\"delta_pred\", \"mean\"),\n",
    "        median_uplift=(\"delta_pred\", \"median\"),\n",
    "        uplift_win_rate=(\"delta_pred\", lambda s: float(np.mean(s > 0))),\n",
    "        downside_rate=(\"delta_pred\", lambda s: float(np.mean(s < 0))),\n",
    "    )\n",
    "    .sort_values(\"mean_uplift\", ascending=False)\n",
    "    .reset_index()\n",
    ")\n",
    "display(scenario_kpi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scenario uplift distribution and average uplift.\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_df = scenario_kpi.sort_values(\"mean_uplift\", ascending=True)\n",
    "plt.barh(plot_df[\"scenario\"], plot_df[\"mean_uplift\"])\n",
    "plt.xlabel(\"Average delta_pred\")\n",
    "plt.title(\"Average predicted LTV uplift by scenario\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "for name in sim_df[\"scenario\"].unique():\n",
    "    chunk = sim_df[sim_df[\"scenario\"] == name]\n",
    "    plt.hist(chunk[\"delta_pred\"], bins=35, alpha=0.4, label=name)\n",
    "plt.axvline(0.0, linestyle=\"--\", linewidth=1)\n",
    "plt.xlabel(\"delta_pred\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Uplift distribution by scenario\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment-level analysis: which company segments benefit most?\n",
    "segment_kpi = (\n",
    "    sim_df.groupby([\"scenario\", \"company_size\"])\n",
    "    .agg(\n",
    "        mean_uplift=(\"delta_pred\", \"mean\"),\n",
    "        uplift_win_rate=(\"delta_pred\", lambda s: float(np.mean(s > 0))),\n",
    "        n_rows=(\"row_id\", \"count\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values([\"scenario\", \"mean_uplift\"], ascending=[True, False])\n",
    ")\n",
    "display(segment_kpi.head(20))\n",
    "\n",
    "pivot = segment_kpi.pivot(index=\"company_size\", columns=\"scenario\", values=\"mean_uplift\")\n",
    "pivot.plot(kind=\"bar\", figsize=(9, 5))\n",
    "plt.ylabel(\"Average delta_pred\")\n",
    "plt.title(\"Segment-level uplift by scenario\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operational shortlist: most positively impacted accounts for selected scenario.\n",
    "focus_scenario = \"combined_program\"\n",
    "selected_cols = [\n",
    "    \"row_id\",\n",
    "    \"company_size\",\n",
    "    \"login_days\",\n",
    "    \"feature_usage_count\",\n",
    "    \"support_tickets\",\n",
    "    \"base_pred\",\n",
    "    \"scenario_pred\",\n",
    "    \"delta_pred\",\n",
    "]\n",
    "top_impacted = (\n",
    "    sim_df[sim_df[\"scenario\"] == focus_scenario]\n",
    "    .sort_values(\"delta_pred\", ascending=False)\n",
    "    .head(20)[selected_cols]\n",
    ")\n",
    "display(top_impacted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {},
   "source": [
    "# Operational shortlist: most positively impacted accounts for selected scenario.\n",
    "focus_scenario = \"combined_program\"\n",
    "selected_cols = [\n",
    "    \"row_id\",\n",
    "    \"company_size\",\n",
    "    \"login_days\",\n",
    "    \"feature_usage_count\",\n",
    "    \"support_tickets\",\n",
    "    \"base_pred\",\n",
    "    \"scenario_pred\",\n",
    "    \"delta_pred\",\n",
    "]\n",
    "top_impacted = (\n",
    "    sim_df[sim_df[\"scenario\"] == focus_scenario]\n",
    "    .sort_values(\"delta_pred\", ascending=False)\n",
    "    .head(20)[selected_cols]\n",
    ")\n",
    "display(top_impacted)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}