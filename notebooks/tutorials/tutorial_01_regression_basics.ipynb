{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Concept Primer\nWhat this method is for, when to use it, and key assumptions."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Config Guide\n| Parameter | Why it matters | Recommended starting point |\n|---|---|---|\n| `task.type` | Chooses learning objective | Match your prediction problem |\n| `split.n_splits` | Controls CV variance/bias tradeoff | 4-5 for most tabular tasks |\n| `train.metrics` | Governs optimization visibility | Include one primary + one robustness metric |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Result Interpretation\nRead metrics together with plots; avoid single-metric decisions."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## If-Then Sensitivity\nTry one parameter change and compare outcome direction before broad tuning."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Common Pitfalls\nWatch for leakage, unstable splits, and train/test drift before model selection."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Further Reading\n- README notebook section\n- `DESIGN_BLUEPRINT.md` 13.4\n- API reference under `veldra.api`"
  },
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# Regression Analysis Workflow with Veldra\n",
    "\n",
    "This notebook demonstrates an end-to-end regression analysis using `veldra.api`.\n",
    "It covers training, evaluation, prediction diagnostics, feature importance, optional SHAP,\n",
    "simulation, and export."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This notebook generates synthetic train/test data internally.\n",
    "No external CSV preparation step is required.\n",
    "\n",
    "SHAP-like contribution values are computed with LightGBM `pred_contrib=True`.\n",
    "No extra SHAP package is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143c39c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from veldra.api import Artifact, evaluate, export, fit, predict, simulate\n",
    "\n",
    "\n",
    "# Resolve repository root so this notebook works from any launch directory.\n",
    "def _resolve_repo_root(start: Path) -> Path:\n",
    "    current = start.resolve()\n",
    "    candidates = [current, *current.parents]\n",
    "    for base in candidates:\n",
    "        if (base / \"pyproject.toml\").exists() and (base / \"examples\").exists():\n",
    "            return base\n",
    "    return start.resolve()\n",
    "\n",
    "ROOT = _resolve_repo_root(Path.cwd())\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "OUT_DIR = ROOT / \"examples\" / \"out\" / \"notebook_regression\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TARGET_COL = \"target_ltv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc81463d",
   "metadata": {},
   "source": [
    "## Synthetic Data Generators\n",
    "\n",
    "Define in-notebook generators so this workflow is self-contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbdb061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data generators used in this notebook.\n",
    "def generate_saas_ltv_data(n_samples: int = 2000, random_state: int = 42) -> pd.DataFrame:\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    company_size = rng.choice([\"Small\", \"Medium\", \"Enterprise\"], size=n_samples, p=[0.6, 0.3, 0.1])\n",
    "    login_days = np.clip(rng.normal(10, 8, n_samples), 0, 30).astype(int)\n",
    "    feature_usage = rng.exponential(scale=50, size=n_samples).astype(int)\n",
    "    support_tickets = np.clip(\n",
    "        rng.poisson(lam=np.maximum(login_days / 10, 0.1)) + rng.integers(-1, 2, n_samples),\n",
    "        0,\n",
    "        10,\n",
    "    )\n",
    "    nps = rng.choice(\n",
    "        np.arange(11),\n",
    "        size=n_samples,\n",
    "        p=[0.05, 0.05, 0.05, 0.05, 0.05, 0.10, 0.10, 0.15, 0.20, 0.10, 0.10],\n",
    "    ).astype(float)\n",
    "    nps[rng.random(n_samples) < 0.3] = np.nan\n",
    "    base_ltv_map = {\"Small\": 1000, \"Medium\": 5000, \"Enterprise\": 20000}\n",
    "    base_val = np.array([base_ltv_map[s] for s in company_size], dtype=float)\n",
    "    effect_login = 500 * np.log1p(login_days)\n",
    "    is_enterprise = (company_size == \"Enterprise\").astype(int)\n",
    "    effect_usage = (feature_usage * 10) + (feature_usage * is_enterprise * 50)\n",
    "    nps_filled = np.nan_to_num(nps, nan=7.0)\n",
    "    effect_support = -500 * support_tickets\n",
    "    happy_support = (support_tickets > 0) & (nps_filled >= 9)\n",
    "    effect_support[happy_support] = 1000 * support_tickets[happy_support]\n",
    "    noise = rng.normal(0, 0.2, n_samples)\n",
    "    log_ltv = np.log(base_val) + (effect_login + effect_usage + effect_support) / 5000 + noise\n",
    "    target_ltv = np.round(np.exp(log_ltv), -2)\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"company_size\": company_size,\n",
    "            \"login_days\": login_days,\n",
    "            \"feature_usage_count\": feature_usage,\n",
    "            \"support_tickets\": support_tickets,\n",
    "            \"nps_score\": nps,\n",
    "            \"target_ltv\": target_ltv,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_drifted_data(n_samples: int = 1000, random_state: int = 99) -> pd.DataFrame:\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    company_size = rng.choice([\"Small\", \"Medium\", \"Enterprise\"], size=n_samples, p=[0.6, 0.3, 0.1])\n",
    "    login_days = np.clip(rng.normal(8, 6, n_samples), 0, 30).astype(int)\n",
    "    feature_usage = rng.exponential(scale=50, size=n_samples).astype(int)\n",
    "    support_tickets = np.clip(\n",
    "        rng.poisson(lam=np.maximum(login_days / 10, 0.1)) + rng.integers(-1, 2, n_samples),\n",
    "        0,\n",
    "        10,\n",
    "    )\n",
    "    nps = rng.choice(\n",
    "        np.arange(11),\n",
    "        size=n_samples,\n",
    "        p=[0.05, 0.05, 0.05, 0.05, 0.05, 0.10, 0.10, 0.15, 0.20, 0.10, 0.10],\n",
    "    ).astype(float)\n",
    "    nps[rng.random(n_samples) < 0.3] = np.nan\n",
    "    base_ltv_map = {\"Small\": 1000, \"Medium\": 5000, \"Enterprise\": 12000}\n",
    "    base_val = np.array([base_ltv_map[s] for s in company_size], dtype=float)\n",
    "    effect_login = 100 * np.log1p(login_days)\n",
    "    is_enterprise = (company_size == \"Enterprise\").astype(int)\n",
    "    effect_usage = (feature_usage * 10) + (feature_usage * is_enterprise * 50)\n",
    "    nps_filled = np.nan_to_num(nps, nan=7.0)\n",
    "    effect_support = -500 * support_tickets\n",
    "    happy_support = (support_tickets > 0) & (nps_filled >= 9)\n",
    "    effect_support[happy_support] = 1000 * support_tickets[happy_support]\n",
    "    noise = rng.normal(0, 0.3, n_samples)\n",
    "    log_ltv = np.log(base_val) + (effect_login + effect_usage + effect_support) / 5000 + noise\n",
    "    target_ltv = np.round(np.exp(log_ltv), -2)\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"company_size\": company_size,\n",
    "            \"login_days\": login_days,\n",
    "            \"feature_usage_count\": feature_usage,\n",
    "            \"support_tickets\": support_tickets,\n",
    "            \"nps_score\": nps,\n",
    "            \"target_ltv\": target_ltv,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee748356",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Generate base/drift datasets and prepare LightGBM-native modeling frames (no one-hot encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311d6d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train/test synthetic datasets.\n",
    "train_df = generate_saas_ltv_data(n_samples=2000, random_state=42)\n",
    "test_df = generate_drifted_data(n_samples=1000, random_state=99)\n",
    "\n",
    "display(train_df.head())\n",
    "display(test_df.head())\n",
    "display(train_df.describe(include=\"all\").transpose().head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2419b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare LightGBM-native frames and persist them as Parquet (categorical dtypes preserved).\n",
    "def _prepare_model_frame(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if target_col not in out.columns:\n",
    "        raise ValueError(f\"Missing target column: {target_col}\")\n",
    "    for col in out.drop(columns=[target_col]).select_dtypes(include=[\"object\", \"string\"]).columns:\n",
    "        out[col] = out[col].astype(\"category\")\n",
    "    out[target_col] = out[target_col].to_numpy(dtype=float)\n",
    "    return out\n",
    "\n",
    "train_model_df = _prepare_model_frame(train_df, TARGET_COL)\n",
    "test_model_df = _prepare_model_frame(test_df, TARGET_COL)\n",
    "\n",
    "# Align categorical levels to train set to keep train/test feature space consistent.\n",
    "for col in train_model_df.drop(columns=[TARGET_COL]).select_dtypes(include=[\"category\"]).columns:\n",
    "    if col in test_model_df.columns:\n",
    "        test_model_df[col] = test_model_df[col].astype(\"category\")\n",
    "        test_model_df[col] = test_model_df[col].cat.set_categories(\n",
    "            train_model_df[col].cat.categories\n",
    "        )\n",
    "\n",
    "TRAIN_MODEL_PATH = OUT_DIR / \"train_model_prepared.parquet\"\n",
    "TEST_MODEL_PATH = OUT_DIR / \"test_model_prepared.parquet\"\n",
    "train_model_df.to_parquet(TRAIN_MODEL_PATH, index=False)\n",
    "test_model_df.to_parquet(TEST_MODEL_PATH, index=False)\n",
    "\n",
    "print(f\"prepared_train_parquet={TRAIN_MODEL_PATH}\")\n",
    "print(f\"prepared_test_parquet={TEST_MODEL_PATH}\")\n",
    "display(train_model_df.head())\n",
    "display(test_model_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f43df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for prediction diagnostics and model interpretation.\n",
    "def build_pred_df(\n",
    "    df: pd.DataFrame,\n",
    "    pred: np.ndarray,\n",
    "    split_name: str,\n",
    "    target_col: str,\n",
    ") -> pd.DataFrame:\n",
    "    out = pd.DataFrame(\n",
    "        {\n",
    "            \"actual\": df[target_col].to_numpy(dtype=float),\n",
    "            \"pred\": np.asarray(pred, dtype=float),\n",
    "        },\n",
    "        index=df.index,\n",
    "    )\n",
    "    out[\"error\"] = out[\"pred\"] - out[\"actual\"]\n",
    "    out[\"abs_error\"] = out[\"error\"].abs()\n",
    "    out[\"split\"] = split_name\n",
    "    return out.reset_index(names=\"row_id\")\n",
    "\n",
    "\n",
    "def plot_actual_vs_pred(pred_df: pd.DataFrame) -> None:\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    for split_name in pred_df[\"split\"].unique():\n",
    "        chunk = pred_df[pred_df[\"split\"] == split_name]\n",
    "        plt.scatter(chunk[\"actual\"], chunk[\"pred\"], s=12, alpha=0.5, label=split_name)\n",
    "    min_v = float(min(pred_df[\"actual\"].min(), pred_df[\"pred\"].min()))\n",
    "    max_v = float(max(pred_df[\"actual\"].max(), pred_df[\"pred\"].max()))\n",
    "    plt.plot([min_v, max_v], [min_v, max_v], linestyle=\"--\", linewidth=1)\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(\"Actual vs Predicted (Train/Test)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_error_distribution(pred_df: pd.DataFrame) -> None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    for split_name in pred_df[\"split\"].unique():\n",
    "        chunk = pred_df[pred_df[\"split\"] == split_name]\n",
    "        axes[0].hist(chunk[\"error\"], bins=30, alpha=0.4, label=split_name)\n",
    "    axes[0].set_title(\"Error Histogram\")\n",
    "    axes[0].set_xlabel(\"Pred - Actual\")\n",
    "    axes[0].legend()\n",
    "    pred_df.boxplot(column=\"error\", by=\"split\", ax=axes[1])\n",
    "    axes[1].set_title(\"Error Boxplot by Split\")\n",
    "    axes[1].set_xlabel(\"Split\")\n",
    "    axes[1].set_ylabel(\"Pred - Actual\")\n",
    "    plt.suptitle(\"\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compute_lgb_importance(artifact: Artifact) -> pd.DataFrame:\n",
    "    booster = lgb.Booster(model_str=artifact.model_text)\n",
    "    feature_names = booster.feature_name()\n",
    "    gain = booster.feature_importance(importance_type=\"gain\")\n",
    "    split = booster.feature_importance(importance_type=\"split\")\n",
    "    return (\n",
    "        pd.DataFrame({\"feature\": feature_names, \"importance_gain\": gain, \"importance_split\": split})\n",
    "        .sort_values(\"importance_gain\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_shap_summary(artifact: Artifact, x_sample: pd.DataFrame) -> pd.DataFrame:\n",
    "    booster = lgb.Booster(model_str=artifact.model_text)\n",
    "    contrib = np.asarray(booster.predict(x_sample, pred_contrib=True), dtype=float)\n",
    "    n_features = x_sample.shape[1]\n",
    "    if contrib.ndim != 2 or contrib.shape[1] < n_features:\n",
    "        raise ValueError(\"Unexpected pred_contrib output shape from LightGBM booster.\")\n",
    "    shap_arr = contrib[:, :n_features]\n",
    "    mean_abs = np.mean(np.abs(shap_arr), axis=0)\n",
    "    return (\n",
    "        pd.DataFrame({\"feature\": x_sample.columns.tolist(), \"mean_abs_shap\": mean_abs})\n",
    "        .sort_values(\"mean_abs_shap\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a32d07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RunConfig and train a regression artifact.\n",
    "config = {\n",
    "    \"config_version\": 1,\n",
    "    \"task\": {\"type\": \"regression\"},\n",
    "    \"data\": {\"path\": str(TRAIN_MODEL_PATH), \"target\": TARGET_COL},\n",
    "    \"split\": {\"type\": \"kfold\", \"n_splits\": 5, \"seed\": 42},\n",
    "\"train\": {\n",
    "        \"seed\": 42,\n",
    "        \"num_boost_round\": 2000,\n",
    "        \"early_stopping_rounds\": 200,\n",
    "        \"early_stopping_validation_fraction\": 0.2,\n",
    "        \"auto_num_leaves\": True,\n",
    "        \"num_leaves_ratio\": 1.0,\n",
    "        \"min_data_in_leaf_ratio\": 0.01,\n",
    "        \"min_data_in_bin_ratio\": 0.01,\n",
    "        \"metrics\": [\"rmse\", \"mae\"],\n",
    "        \"lgb_params\": {\n",
    "            \"learning_rate\": 0.01,\n",
    "            \"max_bin\": 255,\n",
    "            \"max_depth\": 10,\n",
    "            \"feature_fraction\": 1.0,\n",
    "            \"bagging_fraction\": 1.0,\n",
    "            \"bagging_freq\": 0,\n",
    "            \"lambda_l1\": 0.0,\n",
    "            \"lambda_l2\": 0.000001,\n",
    "            \"min_child_samples\": 20,\n",
    "            \"first_metric_only\": True,\n",
    "        },\n",
    "    },\n",
    "    \"export\": {\"artifact_dir\": str(OUT_DIR / \"artifacts\")},\n",
    "}\n",
    "\n",
    "run_result = fit(config)\n",
    "run_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f022a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load artifact and evaluate on test data.\n",
    "artifact = Artifact.load(run_result.artifact_path)\n",
    "eval_result = evaluate(artifact, test_model_df)\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcac877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train/test predictions and build comparison table.\n",
    "train_x = train_model_df.drop(columns=[TARGET_COL])\n",
    "test_x = test_model_df.drop(columns=[TARGET_COL])\n",
    "\n",
    "train_pred = predict(artifact, train_x).data\n",
    "test_pred = predict(artifact, test_x).data\n",
    "\n",
    "train_pred_df = build_pred_df(train_model_df, train_pred, \"train\", TARGET_COL)\n",
    "test_pred_df = build_pred_df(test_model_df, test_pred, \"test\", TARGET_COL)\n",
    "pred_comp_df = pd.concat([train_pred_df, test_pred_df], ignore_index=True)\n",
    "display(pred_comp_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ee024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction quality and error distributions.\n",
    "plot_actual_vs_pred(pred_comp_df)\n",
    "plot_error_distribution(pred_comp_df)\n",
    "\n",
    "error_summary = (\n",
    "    pred_comp_df.groupby(\"split\")[\"abs_error\"]\n",
    "    .agg([\"mean\", \"median\", \"max\"])\n",
    "    .rename(columns={\"mean\": \"mae_like\", \"max\": \"max_abs_error\"})\n",
    ")\n",
    "display(error_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84563424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance table and chart.\n",
    "importance_df = compute_lgb_importance(artifact)\n",
    "display(importance_df)\n",
    "\n",
    "top_n = min(20, len(importance_df))\n",
    "plot_df = importance_df.head(top_n).iloc[::-1]\n",
    "plt.figure(figsize=(8, max(4, top_n * 0.25)))\n",
    "plt.barh(plot_df[\"feature\"], plot_df[\"importance_gain\"])\n",
    "plt.title(\"LightGBM Feature Importance (gain)\")\n",
    "plt.xlabel(\"Importance (gain)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d030ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP-like contribution summary via LightGBM pred_contrib.\n",
    "x_sample = train_x.sample(n=min(300, len(train_x)), random_state=42)\n",
    "shap_df = compute_shap_summary(artifact, x_sample)\n",
    "display(shap_df)\n",
    "\n",
    "top_n = min(20, len(shap_df))\n",
    "plot_df = shap_df.head(top_n).iloc[::-1]\n",
    "plt.figure(figsize=(8, max(4, top_n * 0.25)))\n",
    "plt.barh(plot_df[\"feature\"], plot_df[\"mean_abs_shap\"])\n",
    "plt.title(\"SHAP mean(|value|) from LightGBM pred_contrib\")\n",
    "plt.xlabel(\"mean(|contribution|)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eda815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulation scenarios on test features.\n",
    "scenarios = [\n",
    "    {\n",
    "        \"name\": \"usage_up\",\n",
    "        \"actions\": [\n",
    "            {\"op\": \"add\", \"column\": \"feature_usage_count\", \"value\": 20},\n",
    "            {\"op\": \"clip\", \"column\": \"feature_usage_count\", \"min\": 0, \"max\": 500},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"support_down\",\n",
    "        \"actions\": [\n",
    "            {\"op\": \"add\", \"column\": \"support_tickets\", \"value\": -1},\n",
    "            {\"op\": \"clip\", \"column\": \"support_tickets\", \"min\": 0, \"max\": 10},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "simulate_result = simulate(artifact, test_x, scenarios)\n",
    "display(simulate_result.data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e5bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export artifact as a portable Python package.\n",
    "export_result = export(artifact, format=\"python\")\n",
    "export_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Train/Test error comparison completed (table + plots)\n",
    "- Feature importance and optional SHAP visualizations completed\n",
    "- Scenario simulation and Python export completed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}